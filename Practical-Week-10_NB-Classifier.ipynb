{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes' theorem with the \"naive\" assumption of conditional independence between every pair of features given the value of the class variable. Bayes'theorem states the following relationship, given class variable $y$ and dependent feature vector $x_1$ through $x_n$,:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the naive conditional independence assumption, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$; the former is then the relative frequency of class $y$ in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References*:\n",
    "H. Zhang (2004). The optimality of Naive Bayes. Proc. FLAIRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) implements the Gaussian Naive Bayes algorithm for classification on the data sets where features are continuous.   \n",
    "The likelihood of the features is assumed to be Gaussian:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters $\\sigma_y$ and $\\mu_y$  are estimated using maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo\n",
    "In this demo, we show how to build a Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a synthetica 2D dataset\n",
    "X, y = make_classification(n_samples=50, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_classes=3, n_clusters_per_class=1, \n",
    "                           weights=None, flip_y=0.01, class_sep=0.5, hypercube=True,\n",
    "                           shift=0.0, scale=1.0, shuffle=True, random_state=42)\n",
    "\n",
    "# Data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "# Visualize the generated data\n",
    "colors = ['blue', 'yellow', 'green']\n",
    "for i, color in enumerate(colors):\n",
    "    plt.scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], c=color)\n",
    "plt.scatter(X_test[:, 0], X_test[:,1], c='red', marker='x', label='Testing Data')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n",
      "[[ 0.473217   -2.92751099]\n",
      " [-0.13251585  0.55279599]\n",
      " [ 0.10118627 -1.2450102 ]\n",
      " [ 0.74096317  0.70786564]\n",
      " [ 0.81518332 -0.56023023]\n",
      " [ 0.03812606  0.68954121]\n",
      " [ 0.13025593  0.53262881]\n",
      " [ 0.10383493 -1.81428474]\n",
      " [ 0.66454553 -2.20680262]\n",
      " [ 1.09794267 -2.37877725]\n",
      " [ 0.27749151 -0.854605  ]\n",
      " [-0.53316481 -0.70513853]\n",
      " [ 0.63757741  0.01213484]\n",
      " [ 0.04046135  1.59824301]\n",
      " [ 0.16893175  0.57777706]\n",
      " [ 1.79734745  0.28474816]\n",
      " [ 1.25902255  0.31331681]\n",
      " [ 0.24598286 -0.40419339]\n",
      " [ 0.80229983  0.30773135]\n",
      " [-1.07791165  0.7846769 ]\n",
      " [ 1.31042584  0.19436827]\n",
      " [-0.40911573 -0.49397966]\n",
      " [-0.24888401  0.35671172]\n",
      " [ 0.46062499  0.33753716]\n",
      " [ 0.37015937 -1.80675394]\n",
      " [-0.0209867  -1.14752199]\n",
      " [-0.93394919 -0.41554394]\n",
      " [-0.77955519  0.64076619]\n",
      " [ 0.65883537 -0.52938345]\n",
      " [ 2.10961578  0.16063375]\n",
      " [ 0.79821042 -0.30174085]\n",
      " [-1.26426797  1.04233134]\n",
      " [ 1.55499494  0.24637834]\n",
      " [-0.42413555 -0.4178452 ]\n",
      " [ 0.26824054  0.61720713]\n",
      " [-0.75096812 -0.22755654]\n",
      " [-0.44141363 -0.99279685]\n",
      " [-1.10532366 -0.54174553]\n",
      " [ 0.7178208   0.54839076]\n",
      " [ 0.98185149 -1.81340921]\n",
      " [-1.08536098  0.34484135]\n",
      " [ 1.41765884  0.22358941]\n",
      " [-1.02401946  0.99376658]\n",
      " [ 0.9615424   0.48101337]\n",
      " [-0.28424082 -1.02081839]\n",
      " [ 0.87838387 -1.42667934]\n",
      " [ 0.14645731  0.39639303]\n",
      " [-1.29192246 -0.17421096]\n",
      " [-1.02728125  0.86974824]\n",
      " [ 1.29428097 -1.06070323]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "[2 1 2 0 0 1 1 0 0 0 2 2 0 0 1 1 0 2 1 1 1 2 0 0 2 2 2 1 0 1 0 1 1 2 1 2 2\n",
      " 2 0 0 2 1 1 1 0 0 1 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is:  0.8\n"
     ]
    }
   ],
   "source": [
    "# Create and training a Gaussian Naive Bayes classifier model\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to predict testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print('Testing accuracy is: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.325 0.375 0.3  ]\n",
      "Estimated mean for each Gaussian distribution: \n",
      " [[ 0.60903899 -0.56115715]\n",
      " [ 0.39670288  0.51301944]\n",
      " [-0.40161257 -0.83685934]]\n",
      "Estimated variance for each Gaussian distribution: \n",
      " [[0.23233913 1.0483911 ]\n",
      " [0.93521181 0.0662763 ]\n",
      " [0.3309853  0.6755908 ]]\n"
     ]
    }
   ],
   "source": [
    "# Explore the learned probability (model parameters)\n",
    "print('Estimated probability of classess: \\n', clf.class_prior_)\n",
    "print('Estimated mean for each Gaussian distribution: \\n', clf.theta_)\n",
    "print('Estimated variance for each Gaussian distribution: \\n', clf.sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for Class 0 and the first feature, we can have the following Gaussian disribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_0 \\mid Class=0) = \\frac{1}{\\sqrt{2\\pi\\cdot0.2323}} \\exp\\left(-\\frac{(x_0 - 0.6090)^2}{2\\cdot0.2323}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes accuracy: 0.7000 +- 0.1612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use 10-fold cross validation to show a more robust prediction accuracy\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = GaussianNB()\n",
    "scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "print('Gaussian Naive Bayes accuracy: %.4f +- %.4f\\n' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks** - The training data is generated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means estaimated manually: \n",
      " [[-2.         -1.33333333]\n",
      " [ 2.          1.33333333]]\n",
      "Variances estaimated manually: \n",
      " [[0.66666667 0.22222222]\n",
      " [0.66666667 0.22222222]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "# Firstly, let's do the parameter estimation manually without using the model\n",
    "X_0_C_1=X[y==1][:,0]\n",
    "X_1_C_1=X[y==1][:,1]\n",
    "X_0_C_2=X[y==2][:,0]\n",
    "X_1_C_2=X[y==2][:,1]\n",
    "\n",
    "manual_means = np.array([[X_0_C_1.mean(), X_1_C_1.mean()], [X_0_C_2.mean(), X_1_C_2.mean()]])\n",
    "print('Means estaimated manually: \\n', manual_means)\n",
    "manual_vars = np.array([[X_0_C_1.var(), X_1_C_1.var()], [X_0_C_2.var(), X_1_C_2.var()]])\n",
    "print('Variances estaimated manually: \\n', manual_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**: Training a GaussianNB model and print out the learned model parameters (parameters of probability distributions). And check if the learned parameters comply with the manually estimated ones as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model mean: \n",
      " [[-2.         -1.33333333]\n",
      " [ 2.          1.33333333]]\n",
      "Training model variance: \n",
      " [[0.66666667 0.22222223]\n",
      " [0.66666667 0.22222223]]\n"
     ]
    }
   ],
   "source": [
    "# Create and training a Gaussian Naive Bayes classifier model\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "print('Training model mean: \\n', model.theta_)\n",
    "print('Training model variance: \\n', model.sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : The values that were estimated manually and by the model are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**: Predict the label of a data [-0.8,-1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label predicted for [-0.8, -1] is [1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Label predicted for [-0.8, -1] is\",model.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) classification model is suitable for classification with discrete features. To let the model handle to categorical data, we often need to transform the categorical values to numberic ones, through [encoding](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Outlook Humidity    Wind Play\n",
      "0     Sunny     High    Weak   No\n",
      "1     Sunny     High  Strong   No\n",
      "2  Overcast     High    Weak  Yes\n",
      "3      Rain     High    Weak  Yes\n",
      "4      Rain   Normal    Weak  Yes\n",
      "\n",
      "Data shape:  (14, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load the weather data\n",
    "weather_data = pd.read_csv('weather.csv')\n",
    "print(weather_data.head())\n",
    "print('\\nData shape: ', weather_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing and preparation\n",
    "# Firstly, we need to convert the date from being categorical to being numerical\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "coded_data = enc.fit_transform(weather_data)\n",
    "\n",
    "X = coded_data[:, 0:-1]\n",
    "y = coded_data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat and train a model\n",
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = clf_mnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is: %.4f\\n' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.4 0.6]\n",
      "Estimated class-conditional probabilities for each feature: \n",
      " [[0.63636364 0.18181818 0.18181818]\n",
      " [0.41176471 0.29411765 0.29411765]]\n"
     ]
    }
   ],
   "source": [
    "# Explore the learned model parameters (probabilities)\n",
    "# Note that the probabilities are in the logorithmic form. Why? The log-sum-exp trick for underflow of probability products\n",
    "print('Estimated probability of classess: \\n', np.e**clf_mnb.class_log_prior_)\n",
    "print('Estimated class-conditional probabilities for each feature: \\n', np.e**clf_mnb.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks** - The training data is generated as follows. The number of data instances (6) is small while the demensionality of the data is relatively highly (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic data set\n",
    "import numpy as np\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**: Training a MultinomialNB model, and predict the label of a data X_new = [[1,2,1,0,2,3,0,3,2,1,1,3,3,0,4,2,2,0,0,2,2,3,4,4,4,4,0,3,3,\n",
    "          1,1,1,2,3,1,3,0,2,2,0,4,2,4,3,2,0,1,1,1,2,3,0,0,3,4,3,3,4,\n",
    "          2,1,0,0,0,0,4,1,2,0,0,4,4,0,4,1,3,1,1,1,3,1,1,1,4,3,1,1,3,\n",
    "          2,0,0,0,3,4,1,1,4,3,2,3,4]]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class of X_new is [6]\n"
     ]
    }
   ],
   "source": [
    "# Training a MultinomialNB model\n",
    "multimodal = MultinomialNB()\n",
    "multimodal.fit(X,y)\n",
    "\n",
    "# Predict the class of the new data instance\n",
    "X_new = [[1,2,1,0,2,3,0,3,2,1,1,3,3,0,4,2,2,0,0,2,2,3,4,4,4,4,0,3,3,\n",
    "          1,1,1,2,3,1,3,0,2,2,0,4,2,4,3,2,0,1,1,1,2,3,0,0,3,4,3,3,4,\n",
    "          2,1,0,0,0,0,4,1,2,0,0,4,4,0,4,1,3,1,1,1,3,1,1,1,4,3,1,1,3,\n",
    "          2,0,0,0,3,4,1,1,4,3,2,3,4]]\n",
    "predicted_class = multimodal.predict(X_new)\n",
    "print(\"The predicted class of X_new is\",predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**: In our lecture, we discussed that if there is no occurence of some feature values, zero probabilities will appear. To overcome this issue, Laplace correction (smoothing) is proposed, as shown in the follow formula. In the [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) implementation, the parameter 'alpha' controls the way we apply Laplace smoothing. The default value is 'alpha=1.0'. Please create and train a model with no Laplace smoothing for the above data set. Check if there are zero probabilities (note that due to the accuracy issue, zero might be represented as a signficantly small number by the computer), and compare the leaned model parameters (probabilities) with the case 'alpha=1' \n",
    "$$p(x_{yi}|y)=\\frac{N_{yi}+\\alpha}{N_y+{\\alpha}n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train a MultinomialNB model with no Laplace smoothing\n",
    "mm = MultinomialNB(alpha=0)\n",
    "mm.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mma = MultinomialNB(alpha=1)\n",
    "mma.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Process on 'Iris' Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Week 9, we have studied how to use KNN algorithm to do classification task on 'iris' data. Here,we are going to employ the GaussianNB to conduct the same task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data['data'], iris_data['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**：Report the prediction result on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Train a model and do the prediction\n",
    "i_model = GaussianNB()\n",
    "i_model.fit(X_train, y_train)\n",
    "\n",
    "predicted_y = i_model.predict(X_test)\n",
    "y_pred = i_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print('Accuracy is: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**: Compare the prediction accuaracy between KNN clasifier (use the optimal K you've identied) and Gaussian Naive Bayes. Use 10-cross validation to report the accuracy mean and standard deviation (Note this is to ensure the comparison is based on robust performace). Which classifidation mdoel is more accurate on Iris data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each round: \n",
      " [1.         1.         0.90909091 1.         1.         1.\n",
      " 1.         0.81818182 1.         0.90909091]\n",
      "Average accuracy with std deviation is: 0.9636 +- 0.0603\n"
     ]
    }
   ],
   "source": [
    "# Compare the two types of model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 10)\n",
    "scores = cross_val_score(knn, X_train, y_train, cv=10)\n",
    "print('Accuracy for each round: \\n', scores)\n",
    "print('Average accuracy with std deviation is: %.4f +- %.4f' % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: The Gaussian Modelling is giving an accuracy score of 1 whereas when we use the KNN classifier the accuracy comes down to 0.9636. Hence when we can conclude that the results are better for Gaussian Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**: Can we use Multinomial Naive Bayes classifiation model for Iris data? Yes! We can discretize continuous features by use the [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer) method. Note that you can different ways of discrization. Please report the prediction result with the following discritization method. Also, try another discritization method with parameter 'encode=onehot', and report the prediction result. Use 10-cross validation to report the accuracy mean and standard deviation as we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes accuracy: 0.9267 +- 0.0629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Discretize the continous attributes\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "encoder = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "X = encoder.fit_transform(iris_data['data'])\n",
    "y = iris_data['target']\n",
    "\n",
    "multinomial = MultinomialNB()\n",
    "multinomial.fit(X,y)\n",
    "\n",
    "clf1 = GaussianNB()\n",
    "scores = cross_val_score(clf1, X, y, scoring='accuracy', cv=10)\n",
    "print('Gaussian Naive Bayes accuracy: %.4f +- %.4f\\n' % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = KBinsDiscretizer(n_bins=5, encode='onehot', strategy='uniform')\n",
    "X_new = en.fit_transform(iris_data['data'])\n",
    "y_new = iris_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes accuracy with standard deviation: 0.9200 +- 0.0653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(multinomial, X_new, y_new, scoring='accuracy', cv=10)\n",
    "print('Multinomial Naive Bayes accuracy with standard deviation: %.4f +- %.4f\\n' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Predict Human Activity Recognition (HAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this practice exercise is to predict current human activity based on phisiological activity measurements from 53 different features based in the [HAR dataset](http://groupware.les.inf.puc-rio.br/har#sbia_paper_section). The training (`har_train.csv`) and test (`har_validate.csv`) datasets are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**: Build a Naive Bayes model, predict on the test dataset and compute the [confusion matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62). Note: Please refer to the [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). This is a check point question for this week's workshop. You need to report accuracy_scores on train and test set of Human Activity Recognition dataset. Also provide confusion matrix on test set and provide a brief interpretation of your results based on accuracy scores and confusion matrix (which class is misclassified into what). Your description should not be more than a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "har_train = pd.read_csv(\"har_train.csv\")\n",
    "har_test = pd.read_csv(\"har_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['classe', 'roll_belt', 'pitch_belt', 'yaw_belt', 'total_accel_belt',\n",
      "       'gyros_belt_x', 'gyros_belt_y', 'gyros_belt_z', 'accel_belt_x',\n",
      "       'accel_belt_y', 'accel_belt_z', 'magnet_belt_x', 'magnet_belt_y',\n",
      "       'magnet_belt_z', 'roll_arm', 'pitch_arm', 'yaw_arm', 'total_accel_arm',\n",
      "       'gyros_arm_x', 'gyros_arm_y', 'gyros_arm_z', 'accel_arm_x',\n",
      "       'accel_arm_y', 'accel_arm_z', 'magnet_arm_x', 'magnet_arm_y',\n",
      "       'magnet_arm_z', 'roll_dumbbell', 'pitch_dumbbell', 'yaw_dumbbell',\n",
      "       'total_accel_dumbbell', 'gyros_dumbbell_x', 'gyros_dumbbell_y',\n",
      "       'gyros_dumbbell_z', 'accel_dumbbell_x', 'accel_dumbbell_y',\n",
      "       'accel_dumbbell_z', 'magnet_dumbbell_x', 'magnet_dumbbell_y',\n",
      "       'magnet_dumbbell_z', 'roll_forearm', 'pitch_forearm', 'yaw_forearm',\n",
      "       'total_accel_forearm', 'gyros_forearm_x', 'gyros_forearm_y',\n",
      "       'gyros_forearm_z', 'accel_forearm_x', 'accel_forearm_y',\n",
      "       'accel_forearm_z', 'magnet_forearm_x', 'magnet_forearm_y',\n",
      "       'magnet_forearm_z'],\n",
      "      dtype='object')\n",
      "\n",
      " {'D', 'C', 'A', 'B', 'E'}\n"
     ]
    }
   ],
   "source": [
    "# checking all 53 column names in the dataset\n",
    "print(har_train.columns)\n",
    "\n",
    "# The variable to predict (or label) is \"classe\" column. \n",
    "# checking number of classes in classe column\n",
    "print(\"\\n\",set(har_train['classe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [13737, 5885]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-76b2e783759a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhar_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhar_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhar_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhar_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2116\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2118\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \"\"\"\n\u001b[0;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 212\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [13737, 5885]"
     ]
    }
   ],
   "source": [
    "# Separate features and class labels\n",
    "X=har_train['classe']\n",
    "y=har_test['classe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [13737, 5885]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-934dd3c98a64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Gaussian Naive Bayes accuracy: %.4f +- %.4f\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    391\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \"\"\"\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \"\"\"\n\u001b[0;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 212\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [13737, 5885]"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = GaussianNB()\n",
    "score = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "print('Gaussian Naive Bayes accuracy: %.4f +- %.4f\\n' % (score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporting accuracy score and confusion matrix on test set\n",
    "[code...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
